{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadamard Multitask GP Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to perform \"Hadamard\" multitask regression. \n",
    "This differs from the [multitask gp regression example notebook](./Multitask_GP_Regression.ipynb) in one key way:\n",
    "\n",
    "- Here, we assume that we have observations for **one task per input**. For each input, we specify the task of the input that we observe. (The kernel that we learn is expressed as a Hadamard product of an input kernel and a task kernel)\n",
    "- In the other notebook, we assume that we observe all tasks per input. (The kernel in that notebook is the Kronecker product of an input kernel and a task kernel).\n",
    "\n",
    "Multitask regression, first introduced in [this paper](https://papers.nips.cc/paper/3189-multi-task-gaussian-process-prediction.pdf) learns similarities in the outputs simultaneously. It's useful when you are performing regression on multiple functions that share the same inputs, especially if they have similarities (such as being sinusodial).\n",
    "\n",
    "Given inputs $x$ and $x'$, and tasks $i$ and $j$, the covariance between two datapoints and two tasks is given by\n",
    "\n",
    "$$  k([x, i], [x', j]) = k_\\text{inputs}(x, x') * k_\\text{tasks}(i, j)\n",
    "$$\n",
    "\n",
    "where $k_\\text{inputs}$ is a standard kernel (e.g. RBF) that operates on the inputs.\n",
    "$k_\\text{task}$ is a special kernel - the `IndexKernel` - which is a lookup table containing inter-task covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training data\n",
    "\n",
    "In the next cell, we set up the training data for this example. For each task we'll be using 50 random points on [0,1), which we evaluate the function on and add Gaussian noise to get the training labels. Note that different inputs are used for each task.\n",
    "\n",
    "We'll have two functions - a sine function (y1) and a cosine function (y2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NOISES = [math.sqrt(0.3), math.sqrt(0.1)]\n",
    "torch.manual_seed(1)\n",
    "train_x1 = torch.rand(20)\n",
    "train_x2 = torch.rand(20)\n",
    "\n",
    "train_i_task1 = torch.full((train_x1.shape[0],1), dtype=torch.long, fill_value=0)\n",
    "train_i_task2 = torch.full((train_x2.shape[0],1), dtype=torch.long, fill_value=1)\n",
    "\n",
    "train_f1 = torch.sin(train_x1 * (2 * math.pi))\n",
    "train_f2 = torch.cos(train_x2 * (2 * math.pi))\n",
    "\n",
    "train_noise1 = torch.randn(train_f1.size())\n",
    "train_noise2 = torch.randn(train_f2.size())\n",
    "\n",
    "full_train_x = torch.cat([train_x1, train_x2])\n",
    "full_train_i = torch.cat([train_i_task1, train_i_task2])\n",
    "full_train_f = torch.cat([train_f1, train_f2])\n",
    "full_train_noise = torch.cat([TASK_NOISES[0] * train_noise1, TASK_NOISES[1] * train_noise2])\n",
    "full_train_y = full_train_f + full_train_noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a Hadamard multitask model\n",
    "\n",
    "The model should be somewhat similar to the `ExactGP` model in the [simple regression example](../01_Exact_GPs/Simple_GP_Regression.ipynb).\n",
    "\n",
    "The differences:\n",
    "\n",
    "1. The model takes two input: the inputs (x) and indices. The indices indicate which task the observation is for.\n",
    "2. Rather than just using a RBFKernel, we're using that in conjunction with a IndexKernel.\n",
    "3. We don't use a ScaleKernel, since the IndexKernel will do some scaling for us. (This way we're not overparameterizing the kernel.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel()\n",
    "        \n",
    "        # We learn an IndexKernel for 2 tasks\n",
    "        # (so we'll actually learn 2x2=4 tasks with correlations)\n",
    "        self.task_covar_module = gpytorch.kernels.IndexKernel(num_tasks=2, rank=1)\n",
    "\n",
    "    def forward(self,x,i):\n",
    "        mean_x = self.mean_module(x)\n",
    "        \n",
    "        # Get input-input covariance\n",
    "        covar_x = self.covar_module(x)\n",
    "        # Get task-task covariance\n",
    "        covar_i = self.task_covar_module(i)\n",
    "        # Multiply the two together to get the covariance we want\n",
    "        covar = covar_x.mul(covar_i)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "In the next cell, we handle using Type-II MLE to train the hyperparameters of the Gaussian process.\n",
    "\n",
    "See the [simple regression example](../01_Exact_GPs/Simple_GP_Regression.ipynb) for more info on this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iterations = 2 if smoke_test else 100\n",
    "\n",
    "# We define the training loop in a function, which will let us use\n",
    "# it again later for a different likelihood.\n",
    "def train_model(train_data, likelihood_cls: type[gpytorch.likelihoods.Likelihood]):\n",
    "    likelihood = likelihood_cls(num_tasks=2)\n",
    "    (train_x, train_i), train_y = train_data\n",
    "    # Here we have two terms that we're passing in as train_inputs\n",
    "    model = MultitaskGPModel((train_x, train_i), train_y, likelihood)\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x, train_i)\n",
    "        loss = -mll(output, train_y, [train_i])\n",
    "        loss.backward()\n",
    "        if (i + 1) % 25 == 0:\n",
    "            print(f'Iter {i+1}/{training_iterations} - Loss: {loss.item():.3f}')\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # Set into eval mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "model, likelihood = train_model(\n",
    "    ((full_train_x, full_train_i), full_train_y), \n",
    "    gpytorch.likelihoods.GaussianLikelihood\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plots\n",
    "f, (y1_ax, y2_ax) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# Test points every 0.02 in [0,1]\n",
    "test_x = torch.linspace(0, 1, 51)\n",
    "test_i_task1 = torch.full((test_x.shape[0],1), dtype=torch.long, fill_value=0)\n",
    "test_i_task2 = torch.full((test_x.shape[0],1), dtype=torch.long, fill_value=1)\n",
    "\n",
    "# Make predictions - one task at a time\n",
    "# We control the task we cae about using the indices\n",
    "\n",
    "# The gpytorch.settings.fast_pred_var flag activates LOVE (for fast variances)\n",
    "# See https://arxiv.org/abs/1803.06058\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred_y1 = likelihood(model(test_x, test_i_task1), [test_i_task1])\n",
    "    observed_pred_y2 = likelihood(model(test_x, test_i_task2), [test_i_task2])\n",
    "\n",
    "\n",
    "# Define plotting function\n",
    "def ax_plot(ax, train_y, train_x, rand_var, title):\n",
    "    # Get lower and upper confidence bounds\n",
    "    lower, upper = rand_var.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.detach().numpy(), train_y.detach().numpy(), 'k*')\n",
    "    # Predictive mean as blue line\n",
    "    ax.plot(test_x.detach().numpy(), rand_var.mean.detach().numpy(), 'b')\n",
    "    # Shade in confidence \n",
    "    ax.fill_between(test_x.detach().numpy(), lower.detach().numpy(), upper.detach().numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Plot both tasks\n",
    "train_y1 = train_f1 + TASK_NOISES[0] * train_noise1\n",
    "train_y2 = train_f2 + TASK_NOISES[1] * train_noise2\n",
    "ax_plot(y1_ax, train_y1, train_x1, observed_pred_y1, fr'Task 1 ($\\sigma_y^2 = {TASK_NOISES[0]}$)')\n",
    "ax_plot(y2_ax, train_y2, train_x2, observed_pred_y2, fr'Task 2 ($\\sigma_y^2 = {TASK_NOISES[1]}$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-specific Noise\n",
    "\n",
    "In this notebook so far, we assumed that each task had the same noise. However, \n",
    "this may be too strong an assumption. In this section, we use the `HadamardGaussianLikelihood`\n",
    "to learn independent noises for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hd, likelihood_hd = train_model(\n",
    "    ((full_train_x, full_train_i), full_train_y), \n",
    "    gpytorch.likelihoods.HadamardGaussianLikelihood\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we compare the models when the tasks have significantly different noises. \n",
    "Note that the training loss achieved using the `HadamardGaussianLikelihood` is lower\n",
    "than using `GaussianLikelihood`, which learns the same noise across all tasks. \n",
    "We also see that the predictive distribution for task 2 is much tighter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred_y1_hd = likelihood_hd(model_hd(test_x, test_i_task1), [test_i_task1])\n",
    "    observed_pred_y2_hd = likelihood_hd(model_hd(test_x, test_i_task2), [test_i_task2])\n",
    "\n",
    "train_y1 = train_f1 + TASK_NOISES[0] * train_noise1\n",
    "train_y2 = train_f2 + TASK_NOISES[1] * train_noise2\n",
    "\n",
    "fig = plt.figure(figsize=(8, 7))\n",
    "subfigs = fig.subfigures(2, 1)\n",
    "subfigs[0].suptitle('Shared noise')\n",
    "subfigs[1].suptitle('Task-specific noise')\n",
    "\n",
    "for row, (subfig, pred_y1, pred_y2) in enumerate(zip(subfigs, (observed_pred_y1, observed_pred_y1_hd), (observed_pred_y2, observed_pred_y2_hd))):\n",
    "    y1_ax, y2_ax = subfig.subplots(1, 2)\n",
    "    ax_plot(y1_ax, train_y1, train_x1, pred_y1, fr'Task 1 ($\\sigma_y^2 = {TASK_NOISES[0]**2:.2f}$)')\n",
    "    ax_plot(y2_ax, train_y2, train_x2, pred_y2, fr'Task 2 ($\\sigma_y^2 = {TASK_NOISES[1]**2:.2f}$)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure case of task-specific noise\n",
    "\n",
    "The downside to this approach is that, since each task has its own noise, learning\n",
    "the noise parameter requires more data. We demonstrate this failure case below,\n",
    "where each task has the same noise. In the low-data regime, learning a single\n",
    "noise parameter gives accurate results, however the task-specific noises are not accurate,\n",
    "with task 1 overestimating the noise, and task 2 underestimating.\n",
    "\n",
    "This can be mitigated by setting the `noise_prior` argument of the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 7))\n",
    "subfigs = fig.subfigures(2, 1)\n",
    "subfigs[0].suptitle('Shared noise')\n",
    "subfigs[1].suptitle('Task-specific noise')\n",
    "\n",
    "# Reduce the size of the training set to show the effect of independent noises\n",
    "# in low data settings\n",
    "N_train = 10\n",
    "TASK_NOISE = TASK_NOISES[0]\n",
    "full_train_x = torch.cat([train_x1[:N_train], train_x2[:N_train]])\n",
    "full_train_i = torch.cat([train_i_task1[:N_train], train_i_task2[:N_train]])\n",
    "full_train_f = torch.cat([train_f1[:N_train], train_f2[:N_train]])\n",
    "full_train_noise = torch.cat([TASK_NOISE * train_noise1[:N_train], TASK_NOISE * train_noise2[:N_train]])\n",
    "full_train_y = full_train_f + full_train_noise\n",
    "\n",
    "likelihoods = (gpytorch.likelihoods.GaussianLikelihood, gpytorch.likelihoods.HadamardGaussianLikelihood)\n",
    "for row, (subfig, likelihood_cls) in enumerate(zip(subfigs, likelihoods)):\n",
    "    model, likelihood = train_model(\n",
    "        ((full_train_x, full_train_i), full_train_y), \n",
    "        likelihood_cls\n",
    "    )\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred_y1 = likelihood(model(test_x, test_i_task1), [test_i_task1])\n",
    "        observed_pred_y2 = likelihood(model(test_x, test_i_task2), [test_i_task2])\n",
    "\n",
    "    y1_ax, y2_ax = subfig.subplots(1, 2)\n",
    "    train_x1_sub = train_x1[:N_train]\n",
    "    train_x2_sub = train_x2[:N_train]\n",
    "    train_y1_sub = train_f1[:N_train] + TASK_NOISE * train_noise1[:N_train]\n",
    "    train_y2_sub = train_f2[:N_train] + TASK_NOISE * train_noise2[:N_train]\n",
    "    ax_plot(y1_ax, train_y1_sub, train_x1_sub, observed_pred_y1, fr'Task 1 ($\\sigma_y^2 = {TASK_NOISE**2:.2f}$)')\n",
    "    ax_plot(y2_ax, train_y2_sub, train_x2_sub, observed_pred_y2, fr'Task 2 ($\\sigma_y^2 = {TASK_NOISE**2:.2f}$)')\n",
    "\n",
    "    # Print the standard deviation, sigma_y which should be \n",
    "    # close to TASK_NOISE\n",
    "    print(f\"{likelihood.noise=}\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
